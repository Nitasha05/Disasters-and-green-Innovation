{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting by Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/General/CPC_with_labels.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "header = True\n",
    "for chunk in df1:\n",
    "    validid = chunk['Category'].str.contains('Adapt', regex=True)\n",
    "    chunk = chunk[validid]\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/Adapt1.csv', index=False, mode='a', header=header)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPC_with_labels_2.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "header = True\n",
    "for chunk in df2:\n",
    "    validid = chunk['Category'].str.contains('Adapt', regex=True)\n",
    "    chunk = chunk[validid]\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/Adapt2.csv', index=False, mode='a', header=header)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/Adapt1.csv', skipinitialspace=True, header=0)\n",
    "df4 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/Adapt2.csv', skipinitialspace=True, header=0)\n",
    "out = pd.concat([df3,df4])\n",
    "out.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/Adapt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating 327402 unique ids from file 1 and 264595 and unique ids from file 2 to get 591997 unique ids\n"
     ]
    }
   ],
   "source": [
    "print (\"concatenating \" + str(df3['appln_id'].nunique()) + \" unique ids from file 1 and \" + str(df4['appln_id'].nunique()) + \" and unique ids from file 2 to get \" + str(out['appln_id'].nunique()) + \" unique ids\")\n",
    "#~590,000 unique patents for Adapat category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating 191276 unique ids from file 1 and 192832 and unique ids from file 2 to get 384108 unique ids\n",
      "concatenating 24828 unique ids from file 1 and 20048 and unique ids from file 2 to get 44876 unique ids\n",
      "concatenating 86299 unique ids from file 1 and 111194 and unique ids from file 2 to get 197493 unique ids\n",
      "concatenating 5023 unique ids from file 1 and 4532 and unique ids from file 2 to get 9555 unique ids\n",
      "concatenating 96947 unique ids from file 1 and 69017 and unique ids from file 2 to get 165964 unique ids\n",
      "concatenating 12530 unique ids from file 1 and 11928 and unique ids from file 2 to get 24458 unique ids\n",
      "concatenating 81656 unique ids from file 1 and 55862 and unique ids from file 2 to get 137518 unique ids\n",
      "concatenating 103545 unique ids from file 1 and 140769 and unique ids from file 2 to get 244314 unique ids\n",
      "concatenating 2168 unique ids from file 1 and 3020 and unique ids from file 2 to get 5188 unique ids\n",
      "concatenating 61239 unique ids from file 1 and 75555 and unique ids from file 2 to get 136794 unique ids\n",
      "concatenating 42970 unique ids from file 1 and 29223 and unique ids from file 2 to get 72193 unique ids\n",
      "concatenating 5471 unique ids from file 1 and 1652 and unique ids from file 2 to get 7123 unique ids\n",
      "concatenating 30492 unique ids from file 1 and 4933 and unique ids from file 2 to get 35425 unique ids\n",
      "concatenating 30114 unique ids from file 1 and 24791 and unique ids from file 2 to get 54905 unique ids\n",
      "concatenating 32269 unique ids from file 1 and 37707 and unique ids from file 2 to get 69976 unique ids\n",
      "concatenating 255564 unique ids from file 1 and 151990 and unique ids from file 2 to get 407554 unique ids\n",
      "concatenating 2 unique ids from file 1 and 11 and unique ids from file 2 to get 13 unique ids\n",
      "concatenating 16334 unique ids from file 1 and 20720 and unique ids from file 2 to get 37054 unique ids\n",
      "concatenating 16522 unique ids from file 1 and 11435 and unique ids from file 2 to get 27957 unique ids\n",
      "concatenating 3298 unique ids from file 1 and 2465 and unique ids from file 2 to get 5763 unique ids\n",
      "concatenating 54207 unique ids from file 1 and 45321 and unique ids from file 2 to get 99528 unique ids\n",
      "concatenating 2532 unique ids from file 1 and 3527 and unique ids from file 2 to get 6059 unique ids\n",
      "concatenating 0 unique ids from file 1 and 3 and unique ids from file 2 to get 3 unique ids\n",
      "concatenating 3670 unique ids from file 1 and 6147 and unique ids from file 2 to get 9817 unique ids\n",
      "concatenating 579079 unique ids from file 1 and 421336 and unique ids from file 2 to get 1000415 unique ids\n",
      "concatenating 436074 unique ids from file 1 and 348521 and unique ids from file 2 to get 784595 unique ids\n",
      "concatenating 232758 unique ids from file 1 and 129427 and unique ids from file 2 to get 362185 unique ids\n",
      "concatenating 25703 unique ids from file 1 and 50182 and unique ids from file 2 to get 75885 unique ids\n",
      "concatenating 1758243 unique ids from file 1 and 134191 and unique ids from file 2 to get 1892434 unique ids\n",
      "concatenating 2614503 unique ids from file 1 and 575931 and unique ids from file 2 to get 3190434 unique ids\n",
      "concatenating 57957 unique ids from file 1 and 11002 and unique ids from file 2 to get 68959 unique ids\n",
      "concatenating 2 unique ids from file 1 and 39 and unique ids from file 2 to get 41 unique ids\n",
      "concatenating 34939 unique ids from file 1 and 62947 and unique ids from file 2 to get 97886 unique ids\n",
      "concatenating 1831 unique ids from file 1 and 2459 and unique ids from file 2 to get 4290 unique ids\n"
     ]
    }
   ],
   "source": [
    "Category = ['Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/General/CPC_with_labels.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "    header = True\n",
    "    for chunk in df1:\n",
    "        validid = chunk['Category'].str.contains(c, regex=True)\n",
    "        chunk = chunk[validid]\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/' + c +'1.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/General/CPC_with_labels_2.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "    header = True\n",
    "    for chunk in df2:\n",
    "        validid = chunk['Category'].str.contains(c, regex=True)\n",
    "        chunk = chunk[validid]\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/'+ c +'2.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/' +c+ '1.csv', skipinitialspace=True, header=0)\n",
    "    df4 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/' +c+ '2.csv', skipinitialspace=True, header=0)\n",
    "    out = pd.concat([df3,df4])\n",
    "    out.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/' +c+ '.csv', index=False)\n",
    "    print (\"concatenating \" + str(df3['appln_id'].nunique()) + \" unique ids from file 1 and \" + str(df4['appln_id'].nunique()) + \" and unique ids from file 2 to get \" + str(out['appln_id'].nunique()) + \" unique ids\")\n",
    "    del df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt: 591,997 unique ids \n",
    "\n",
    "Build: 384,108 unique ids\n",
    "\n",
    "Sequester: 44,876 unique ids\n",
    "\n",
    "ICT: 197,493 unique ids\n",
    "\n",
    "Geo: 9,555 unique ids\n",
    "\n",
    "Hydro: 165,964 unique ids\n",
    "\n",
    "Ocean: 24,458 unique ids\n",
    "\n",
    "Solar-T: 137,518 unique ids\n",
    "\n",
    "PV: 244,314 unique ids\n",
    "\n",
    "Thermal-PV: 5,188 unique ids\n",
    "\n",
    "Wind: 136,794 unique ids\n",
    "\n",
    "Combust: 72,193 unique ids\n",
    "\n",
    "Fusion: 7,123 unique ids\n",
    "\n",
    "Fission: 35,425 unique ids\n",
    "\n",
    "Electric: 54,905 unique ids\n",
    "\n",
    "Biofuel: 69,976 unique ids\n",
    "\n",
    "Waste: 407,554 unique ids\n",
    "\n",
    "Store- B: 13 unique ids\n",
    "\n",
    "Store - C: 37,054 unique ids\n",
    "\n",
    "Store- T: 27,957 unique ids\n",
    "\n",
    "Store- M: 5,763 unique ids\n",
    "\n",
    "Hydrogen: 99,528 unique ids\n",
    "\n",
    "Other-1: 6,059 unique ids\n",
    "\n",
    "Other-2: 3 unique ids\n",
    "\n",
    "Other-3: 9,817 unique ids\n",
    "\n",
    "Processing: 1,000,415 unique ids\n",
    "\n",
    "Transport: 784,595 unique ids\n",
    "\n",
    "Wastewater: 362,185 unique ids\n",
    "\n",
    "SmrtGrds: 75,885 unique ids\n",
    "\n",
    "Other-4: 1,892,434 unique ids\n",
    "\n",
    "Other-5: 3,190,434 unique ids\n",
    "\n",
    "Nuclear: 68,959 unique ids\n",
    "\n",
    "Non-Fossil: 41 unique ids\n",
    "\n",
    "Enable: 97,886 unique ids\n",
    "\n",
    "Other-6: 4,290 unique ids\n",
    "\n",
    "~10.2 million unique ids (including overlap between categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if all 8.5 million unique Y02 ids are retained\n",
    "import pandas as pd\n",
    "ids=pd.DataFrame()\n",
    "Category = ['Adapt','Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "for c in Category:\n",
    "    df= pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Extras/' +c+ '.csv',skipinitialspace=True, chunksize=500000, header=0)\n",
    "    header = True\n",
    "    for chunk in df:\n",
    "        ids=pd.concat([ids,(chunk[\"appln_id\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17241378, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "ids.to_numpy().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8550374,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ids.to_numpy()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8,550,374 (all ids are retained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
