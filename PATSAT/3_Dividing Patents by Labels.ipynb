{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting by Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPC_with_labels.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "header = True\n",
    "for chunk in df1:\n",
    "    validid = chunk['Category'].str.contains('Adapt', regex=True)\n",
    "    chunk = chunk[validid]\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt1.csv', index=False, mode='a', header=header)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPC_with_labels_2.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "header = True\n",
    "for chunk in df2:\n",
    "    validid = chunk['Category'].str.contains('Adapt', regex=True)\n",
    "    chunk = chunk[validid]\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt2.csv', index=False, mode='a', header=header)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt1.csv', skipinitialspace=True, header=0)\n",
    "df4 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt2.csv', skipinitialspace=True, header=0)\n",
    "out = pd.concat([df3,df4])\n",
    "out.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating 327402 unique ids from file 1 and 264595 and unique ids from file 2 to get 591997 unique ids\n"
     ]
    }
   ],
   "source": [
    "print (\"concatenating \" + str(df3['appln_id'].nunique()) + \" unique ids from file 1 and \" + str(df4['appln_id'].nunique()) + \" and unique ids from file 2 to get \" + str(out['appln_id'].nunique()) + \" unique ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating 191276 unique ids from file 1 and 192832 and unique ids from file 2 to get 384108 unique ids\n",
      "concatenating 24828 unique ids from file 1 and 20048 and unique ids from file 2 to get 44876 unique ids\n",
      "concatenating 86299 unique ids from file 1 and 111194 and unique ids from file 2 to get 197493 unique ids\n",
      "concatenating 5023 unique ids from file 1 and 4532 and unique ids from file 2 to get 9555 unique ids\n",
      "concatenating 96947 unique ids from file 1 and 69017 and unique ids from file 2 to get 165964 unique ids\n",
      "concatenating 12530 unique ids from file 1 and 11928 and unique ids from file 2 to get 24458 unique ids\n",
      "concatenating 81656 unique ids from file 1 and 55862 and unique ids from file 2 to get 137518 unique ids\n",
      "concatenating 103545 unique ids from file 1 and 140769 and unique ids from file 2 to get 244314 unique ids\n",
      "concatenating 2168 unique ids from file 1 and 3020 and unique ids from file 2 to get 5188 unique ids\n",
      "concatenating 61239 unique ids from file 1 and 75555 and unique ids from file 2 to get 136794 unique ids\n",
      "concatenating 42970 unique ids from file 1 and 29223 and unique ids from file 2 to get 72193 unique ids\n",
      "concatenating 5471 unique ids from file 1 and 1652 and unique ids from file 2 to get 7123 unique ids\n",
      "concatenating 30492 unique ids from file 1 and 4933 and unique ids from file 2 to get 35425 unique ids\n",
      "concatenating 30114 unique ids from file 1 and 24791 and unique ids from file 2 to get 54905 unique ids\n",
      "concatenating 32269 unique ids from file 1 and 37707 and unique ids from file 2 to get 69976 unique ids\n",
      "concatenating 255564 unique ids from file 1 and 151990 and unique ids from file 2 to get 407554 unique ids\n",
      "concatenating 2 unique ids from file 1 and 11 and unique ids from file 2 to get 13 unique ids\n",
      "concatenating 16334 unique ids from file 1 and 20720 and unique ids from file 2 to get 37054 unique ids\n",
      "concatenating 16522 unique ids from file 1 and 11435 and unique ids from file 2 to get 27957 unique ids\n",
      "concatenating 3298 unique ids from file 1 and 2465 and unique ids from file 2 to get 5763 unique ids\n",
      "concatenating 54207 unique ids from file 1 and 45321 and unique ids from file 2 to get 99528 unique ids\n",
      "concatenating 2532 unique ids from file 1 and 3527 and unique ids from file 2 to get 6059 unique ids\n",
      "concatenating 0 unique ids from file 1 and 3 and unique ids from file 2 to get 3 unique ids\n",
      "concatenating 3670 unique ids from file 1 and 6147 and unique ids from file 2 to get 9817 unique ids\n",
      "concatenating 579079 unique ids from file 1 and 421336 and unique ids from file 2 to get 1000415 unique ids\n",
      "concatenating 436074 unique ids from file 1 and 348521 and unique ids from file 2 to get 784595 unique ids\n",
      "concatenating 232758 unique ids from file 1 and 129427 and unique ids from file 2 to get 362185 unique ids\n",
      "concatenating 25703 unique ids from file 1 and 50182 and unique ids from file 2 to get 75885 unique ids\n",
      "concatenating 1758243 unique ids from file 1 and 134191 and unique ids from file 2 to get 1892434 unique ids\n",
      "concatenating 2614503 unique ids from file 1 and 575931 and unique ids from file 2 to get 3190434 unique ids\n",
      "concatenating 57957 unique ids from file 1 and 11002 and unique ids from file 2 to get 68959 unique ids\n",
      "concatenating 2 unique ids from file 1 and 39 and unique ids from file 2 to get 41 unique ids\n",
      "concatenating 34939 unique ids from file 1 and 62947 and unique ids from file 2 to get 97886 unique ids\n",
      "concatenating 1831 unique ids from file 1 and 2459 and unique ids from file 2 to get 4290 unique ids\n"
     ]
    }
   ],
   "source": [
    "Category = ['Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPC_with_labels.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "    header = True\n",
    "    for chunk in df1:\n",
    "        validid = chunk['Category'].str.contains(c, regex=True)\n",
    "        chunk = chunk[validid]\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' + c +'1.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPC_with_labels_2.csv',skipinitialspace=True, chunksize=5000, header=0)\n",
    "    header = True\n",
    "    for chunk in df2:\n",
    "        validid = chunk['Category'].str.contains(c, regex=True)\n",
    "        chunk = chunk[validid]\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c +'2.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' +c+ '1.csv', skipinitialspace=True, header=0)\n",
    "    df4 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' +c+ '2.csv', skipinitialspace=True, header=0)\n",
    "    out = pd.concat([df3,df4])\n",
    "    out.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' +c+ '.csv', index=False)\n",
    "    print (\"concatenating \" + str(df3['appln_id'].nunique()) + \" unique ids from file 1 and \" + str(df4['appln_id'].nunique()) + \" and unique ids from file 2 to get \" + str(out['appln_id'].nunique()) + \" unique ids\")\n",
    "    del df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
