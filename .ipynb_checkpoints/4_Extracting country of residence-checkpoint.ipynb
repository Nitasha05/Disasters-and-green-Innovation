{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls207_part01.csv',skipinitialspace=True, header=0)\n",
    "df1['appln_id'].nunique()\n",
    "#84,076,043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['person_id'].nunique()\n",
    "#61,558,031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~84 million unique applications have persons associated with them, and ~62 million unique persons have contributed to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Inventors by Application id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I extract all relevant Person Id's from the larger database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "field = ['appln_id']\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPCcodes_1.csv',skipinitialspace=True, usecols = field) \n",
    "ids = df2.drop_duplicates() #extract relevant unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['person_id', 'appln_id']\n",
    "df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls207_part01.csv',skipinitialspace=True, chunksize=200000, header=0,usecols=fields)\n",
    "header = True\n",
    "chunkid=0\n",
    "for chunk in df3:\n",
    "    chunk = ids.merge(chunk, on='appln_id', how='inner') #filter by ids\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_1.csv', index=False, mode='a', header=header)\n",
    "    print (chunkid)\n",
    "    chunkid = chunkid + 1\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ids, df2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "field = ['appln_id']\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CPCcodes_2.csv',skipinitialspace=True, usecols = field) \n",
    "ids = df2.drop_duplicates() #extract relevant unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['person_id', 'appln_id']\n",
    "df3 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls207_part01.csv',skipinitialspace=True, chunksize=500000, header=0,usecols=fields)\n",
    "header = True\n",
    "chunkid=0\n",
    "for chunk in df3:\n",
    "    chunk = ids.merge(chunk, on='appln_id', how='inner') #filter by ids\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_2.csv', index=False, mode='a', header=header)\n",
    "    print (chunkid)\n",
    "    chunkid = chunkid + 1\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ids, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Countries of Residence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I use Person id's to filter out country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#extract relevant person ids\n",
    "field = ['person_id']\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_1.csv',skipinitialspace=True, usecols = field) \n",
    "ids = df1.drop_duplicates() #extract relevant unique person ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['person_id', 'person_ctry_code']\n",
    "#once again, parse through relevant person ids and residence country code columns\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls226_part01.csv',skipinitialspace=True, chunksize=500000, header=0,usecols=fields)\n",
    "header = True\n",
    "chunkid=0\n",
    "for chunk in df2:\n",
    "    chunk = ids.merge(chunk, on='person_id', how='inner') #filter by person ids\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Residence_1.csv', index=False, mode='a', header=header)\n",
    "    print (chunkid)\n",
    "    chunkid = chunkid + 1\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly for Persons 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "field = ['person_id']\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_2.csv',skipinitialspace=True, usecols = field) \n",
    "ids = df1.drop_duplicates() #extract relevant unique person ids\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "fields = ['person_id', 'person_ctry_code']\n",
    "#once again, parse through relevant person ids and residence country code columns\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls226_part02.csv',skipinitialspace=True, chunksize=500000, header=0,usecols=fields)\n",
    "header = True\n",
    "chunkid=0\n",
    "for chunk in df2:\n",
    "    chunk = ids.merge(chunk, on='person_id', how='inner') #filter by person ids\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Residence_2.csv', index=False, mode='a', header=header)\n",
    "    print (chunkid)\n",
    "    chunkid = chunkid + 1\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#once again, parse through relevant person ids and residence country code columns\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/tls226_part02.csv',skipinitialspace=True, header=0, nrows=100)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging names and country codes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_1.csv',skipinitialspace=True, header=0)\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Residence_1.csv',skipinitialspace=True, header=0)\n",
    "df3 = pd.merge(df1, df2.drop_duplicates() , on=\"person_id\", how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CountryPersons1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_2.csv',skipinitialspace=True, header=0)\n",
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Residence_2.csv',skipinitialspace=True, header=0)\n",
    "df3 = pd.merge(df1, df2.drop_duplicates() , on=\"person_id\", how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CountryPersons2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2, df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide person ids and country codes based on application categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "field = ['appln_id']\n",
    "#first try with one category\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt.csv',skipinitialspace=True, usecols = field) \n",
    "ids = df1.drop_duplicates() #extract relevant unique ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CountryPersons1.csv',skipinitialspace=True, chunksize=500000, header=0)\n",
    "header = True\n",
    "for chunk in df2:\n",
    "    if (chunk['appln_id'].dtype is not ids['appln_id'].dtype):\n",
    "        chunk['appln_id'] = pd.to_numeric(chunk['appln_id'], errors='coerce')\n",
    "        chunk.dropna(subset=['appln_id'], inplace=True)\n",
    "        chunk['appln_id'] = chunk['appln_id'].astype(ids['appln_id'].dtype)\n",
    "    chunk = ids.merge(chunk, on='appln_id', how='left') #filter by ids\n",
    "    chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Adapt_residence.csv', index=False, mode='a', header=header)\n",
    "    header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a for loop for the remaining categories\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Category = ['Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "field = ['appln_id']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c +'.csv',skipinitialspace=True, usecols = field) \n",
    "    ids = df1.drop_duplicates() #extract relevant unique ids\n",
    "    df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CountryPersons1.csv',skipinitialspace=True, chunksize=500000, header=0)\n",
    "    header = True\n",
    "    for chunk in df2:\n",
    "        if (chunk['appln_id'].dtype is not ids['appln_id'].dtype):\n",
    "            chunk['appln_id'] = pd.to_numeric(chunk['appln_id'], errors='coerce')\n",
    "            chunk.dropna(subset=['appln_id'], inplace=True)\n",
    "            chunk['appln_id'] = chunk['appln_id'].astype(ids['appln_id'].dtype)\n",
    "        chunk = ids.merge(chunk, on='appln_id', how='left') #filter by ids\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' + c + '_residence.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    del df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the second half of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "Category = ['Adapt','Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "field = ['appln_id']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c +'.csv',skipinitialspace=True, usecols = field) \n",
    "    ids = df1.drop_duplicates() #extract relevant unique ids\n",
    "    df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/CountryPersons2.csv',skipinitialspace=True, chunksize=500000, header=0)\n",
    "    header = True\n",
    "    for chunk in df2:\n",
    "        if (chunk['appln_id'].dtype is not ids['appln_id'].dtype):\n",
    "            chunk['appln_id'] = pd.to_numeric(chunk['appln_id'], errors='coerce')\n",
    "            chunk.dropna(subset=['appln_id'], inplace=True)\n",
    "            chunk['appln_id'] = chunk['appln_id'].astype(ids['appln_id'].dtype)\n",
    "        chunk = ids.merge(chunk, on='appln_id', how='left') #filter by ids\n",
    "        chunk.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/' + c + '_residence2.csv', index=False, mode='a', header=header)\n",
    "        header = False\n",
    "    del df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#merging tables\n",
    "Category = ['Adapt','Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c + '_residence.csv',skipinitialspace=True) \n",
    "    df2 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c + '_residence2.csv',skipinitialspace=True)\n",
    "    df3 = pd.concat([df1, df2])\n",
    "    df3.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c + '_Residence_complete.csv', encoding='utf-8', index=False)\n",
    "    del df1, df2, df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check maximum investors per record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_1.csv',skipinitialspace=True, header=0)\n",
    "items_counts = df1['appln_id'].value_counts()\n",
    "max_item = items_counts.max()\n",
    "del df1\n",
    "#There are a maximum of 132 inventors associated with a patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/Persons_2.csv',skipinitialspace=True, header=0)\n",
    "items_counts = df1['appln_id'].value_counts()\n",
    "max_item = items_counts.max()\n",
    "del df1\n",
    "#There are a maximum of 134 inventors associated with a patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478696789"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['appln_id'].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appln_id</th>\n",
       "      <th>person_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388188</th>\n",
       "      <td>478696789</td>\n",
       "      <td>5262071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954741</th>\n",
       "      <td>478696789</td>\n",
       "      <td>9208555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954742</th>\n",
       "      <td>478696789</td>\n",
       "      <td>9208557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994200</th>\n",
       "      <td>478696789</td>\n",
       "      <td>9973554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032883</th>\n",
       "      <td>478696789</td>\n",
       "      <td>10490957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383580</th>\n",
       "      <td>478696789</td>\n",
       "      <td>56251998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383581</th>\n",
       "      <td>478696789</td>\n",
       "      <td>56341163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383582</th>\n",
       "      <td>478696789</td>\n",
       "      <td>56512835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383583</th>\n",
       "      <td>478696789</td>\n",
       "      <td>56516749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383584</th>\n",
       "      <td>478696789</td>\n",
       "      <td>56572286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          appln_id  person_id\n",
       "388188   478696789    5262071\n",
       "954741   478696789    9208555\n",
       "954742   478696789    9208557\n",
       "994200   478696789    9973554\n",
       "1032883  478696789   10490957\n",
       "...            ...        ...\n",
       "6383580  478696789   56251998\n",
       "6383581  478696789   56341163\n",
       "6383582  478696789   56512835\n",
       "6383583  478696789   56516749\n",
       "6383584  478696789   56572286\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[df1['appln_id'] == 478696789]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we will randmly pick 5 inventors per application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.groupby('appln_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_counts = df2['appln_id'].value_counts()\n",
    "max_item = items_counts.max()\n",
    "max_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this function and loop over all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitashajhala/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (0,1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "Category = ['Adapt','Build', 'Sequester', 'ICT', 'Geo', 'Hydro', 'Ocean', 'Solar-T', 'PV', 'Thermal-PV', 'Wind', 'Combust', 'Fusion', 'Fission', 'Electric', 'Biofuel', 'Waste', 'Store- B', 'Store - C', 'Store- T', 'Store- M', 'Hydrogen', 'Other-1', 'Other-2', 'Other-3', 'Processing', 'Transport', 'Wastewater', 'SmrtGrds', 'Other-4', 'Other-5', 'Nuclear', 'Non-Fossil', 'Enable','Other-6']\n",
    "for c in Category:\n",
    "    df1 = pd.read_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c + '_Residence_complete.csv')\n",
    "    df2 = df1.groupby('appln_id', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))\n",
    "    df2.to_csv('/Volumes/NJ_4TB/PATSAT/MyFiles/'+ c + '_Residence_concise.csv', encoding='utf-8', index=False)\n",
    "    del df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
